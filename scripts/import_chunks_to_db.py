"""
ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import json
import sys
from pathlib import Path
from datetime import datetime
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Float, text
from sqlalchemy.orm import declarative_base, sessionmaker

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.core.config import settings
from app.services.knowledge_service import knowledge_service

Base = declarative_base()


class Chunk(Base):
    """ãƒãƒ£ãƒ³ã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«"""
    __tablename__ = "chunks"
    
    id = Column(Integer, primary_key=True, index=True)
    chunk_id = Column(String, unique=True, index=True, nullable=False)  # UUID
    file_name = Column(String, index=True, nullable=False)
    file_type = Column(String, index=True, nullable=True)
    chunk_index = Column(Integer, nullable=True)  # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã§ã®ãƒãƒ£ãƒ³ã‚¯ç•ªå·
    text_content = Column(Text, nullable=True)  # ãƒãƒ£ãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹
    text_preview = Column(String, nullable=True)  # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰
    text_length = Column(Integer, nullable=True)  # ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•
    document_id = Column(String, nullable=True)
    ref_doc_id = Column(String, nullable=True)  # å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID
    file_size = Column(Integer, nullable=True)  # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚º
    file_updated_at = Column(DateTime, nullable=True)  # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚
    embedding_json = Column(Text, nullable=True)  # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆJSONå½¢å¼ï¼‰
    embedding_dimension = Column(Integer, nullable=True)  # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã®æ¬¡å…ƒæ•°
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    metadata_json = Column(Text, nullable=True)  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‚’JSONæ–‡å­—åˆ—ã§ä¿å­˜


def import_chunks_to_db():
    """ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    engine = create_engine(settings.database_url, connect_args={"check_same_thread": False})
    Base.metadata.create_all(bind=engine)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    db = SessionLocal()
    
    try:
        # æ—¢å­˜ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤ï¼ˆå†ã‚¤ãƒ³ãƒãƒ¼ãƒˆç”¨ï¼‰
        db.execute(text("DELETE FROM chunks"))
        db.commit()
        print("æ—¢å­˜ã®ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        vector_store_path = Path("./storage/index/default__vector_store.json")
        if not vector_store_path.exists():
            print(f"ã‚¨ãƒ©ãƒ¼: {vector_store_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        print(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {vector_store_path}")
        with open(vector_store_path, 'r', encoding='utf-8') as f:
            vector_store = json.load(f)
        
        # docstoreãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’å–å¾—ã™ã‚‹ãŸã‚ï¼‰
        docstore_path = Path("./storage/index/docstore.json")
        docstore_data = {}
        if docstore_path.exists():
            with open(docstore_path, 'r', encoding='utf-8') as f:
                docstore_json = json.load(f)
                # docstoreã®æ§‹é€ ã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                if 'docstore' in docstore_json and 'data' in docstore_json['docstore']:
                    docstore_data = docstore_json['docstore']['data']
        
        # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        embedding_dict = vector_store.get('embedding_dict', {})
        metadata_dict = vector_store.get('metadata_dict', {})
        text_id_to_ref_doc_id = vector_store.get('text_id_to_ref_doc_id', {})
        
        print(f"ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {len(embedding_dict)}")
        
        # Knowledgeãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        file_content_cache = {}
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        imported_count = 0
        for chunk_id, embedding in embedding_dict.items():
            metadata = metadata_dict.get(chunk_id, {})
            
            # docstoreã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’å–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            text_content = None
            if chunk_id in docstore_data:
                chunk_data = docstore_data[chunk_id]
                if isinstance(chunk_data, dict) and 'text' in chunk_data:
                    text_content = chunk_data['text']
            
            # docstoreã‹ã‚‰å–å¾—ã§ããªã„å ´åˆã€Knowledgeãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å†æ§‹ç¯‰ã‚’è©¦ã¿ã‚‹
            if not text_content:
                file_name = metadata.get('file_name', '')
                chunk_index = metadata.get('chunk_index', None)
                
                if file_name and chunk_index is not None:
                    try:
                        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã€ã¾ãŸã¯èª­ã¿è¾¼ã‚€
                        if file_name not in file_content_cache:
                            file_content = knowledge_service.get_file_content(file_name)
                            file_content_cache[file_name] = file_content['content']
                        
                        file_text = file_content_cache[file_name]
                        
                        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º400æ–‡å­—ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—50æ–‡å­—ã§åˆ†å‰²
                        chunk_size = 400
                        overlap = 50
                        start = chunk_index * (chunk_size - overlap)
                        end = start + chunk_size
                        text_content = file_text[start:end] if start < len(file_text) else None
                    except Exception as e:
                        print(f"Warning: Could not get text for {file_name} chunk {chunk_index}: {e}")
                        text_content = None
            
            # ref_doc_idã‚’å–å¾—
            ref_doc_id = text_id_to_ref_doc_id.get(chunk_id)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
            file_name = metadata.get('file_name', 'unknown')
            file_type = metadata.get('file_type', 'unknown')
            chunk_index = metadata.get('chunk_index', None)
            document_id = metadata.get('document_id') or metadata.get('doc_id') or ref_doc_id
            file_size = metadata.get('file_size', None)
            updated_at = metadata.get('updated_at', None)
            
            # updated_atã‚’DateTimeã«å¤‰æ›
            file_updated_at = None
            if updated_at:
                try:
                    file_updated_at = datetime.fromtimestamp(updated_at)
                except (ValueError, TypeError):
                    pass
            
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä½œæˆï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰
            text_preview = None
            if text_content:
                text_preview = text_content[:200] + "..." if len(text_content) > 200 else text_content
            
            # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
            embedding_vector = embedding_dict.get(chunk_id)
            embedding_json = None
            embedding_dimension = None
            if embedding_vector:
                embedding_json = json.dumps(embedding_vector)
                embedding_dimension = len(embedding_vector) if isinstance(embedding_vector, list) else None
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ
            chunk = Chunk(
                chunk_id=chunk_id,
                file_name=file_name,
                file_type=file_type,
                chunk_index=chunk_index,
                text_content=text_content,
                text_preview=text_preview,
                text_length=len(text_content) if text_content else None,
                document_id=document_id,
                ref_doc_id=ref_doc_id,
                file_size=file_size,
                file_updated_at=file_updated_at,
                embedding_json=embedding_json,
                embedding_dimension=embedding_dimension,
                metadata_json=json.dumps(metadata, ensure_ascii=False)
            )
            
            db.add(chunk)
            imported_count += 1
            
            if imported_count % 10 == 0:
                print(f"  ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­: {imported_count}/{len(embedding_dict)}")
        
        db.commit()
        print(f"\nâœ… å®Œäº†: {imported_count}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚")
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        result = db.execute(text("""
            SELECT 
                file_name,
                COUNT(*) as chunk_count,
                AVG(text_length) as avg_length
            FROM chunks
            GROUP BY file_name
            ORDER BY file_name
        """))
        
        print("\nğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®ãƒãƒ£ãƒ³ã‚¯çµ±è¨ˆ:")
        print("-" * 60)
        for row in result:
            avg_len = row.avg_length if row.avg_length else 0
            print(f"  {row.file_name}: {row.chunk_count}ãƒãƒ£ãƒ³ã‚¯ (å¹³å‡é•·: {avg_len:.0f}æ–‡å­—)")
        
    except Exception as e:
        db.rollback()
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()


if __name__ == "__main__":
    import_chunks_to_db()

